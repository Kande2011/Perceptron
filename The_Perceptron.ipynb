{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t0\t0\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t1\t1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!curl http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
    "!curl http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: train.dat: No such file or directory\n",
      "head: test.dat: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head train.dat\n",
    "!head test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    # dot product of array 1 and array 2\n",
    "    dot_product = 0\n",
    "    for i,j in zip(array1,array2):\n",
    "        dot_product = dot_product + (i * j)\n",
    "    return dot_product\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    # outpout of sigmoid function on x\n",
    "    sig = (1.0 / (1.0 + math.exp(-1.0*x)))\n",
    "    return sig\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    # output of the model\n",
    "    output = sigmoid(dot_product(weight, instance))\n",
    "    return output\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    # prediction of the model\n",
    "    if output(weights, instance) >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    # weight with length of training data\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            # loop each object in training data for an each epoch\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            # loop each weight and update it for row in epoch\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat.txt\")\n",
    "instances_te = read_data(\"test.dat.txt\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "\n",
    "The output of both code snippet is same\n",
    "\n",
    "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "\n",
    "but when we add print for \n",
    "\"weights = train_perceptron(instances_tr, lr, epochs)\"\n",
    "The weights are not accurate with second code snippet:\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "output for weights:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "for first code snippet - \n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "output for weights:  [0.08016249451473897, -0.018803740641657435, -0.02722022427977521, -0.02235226664042487, -0.02336209277151821, -0.030082783050668655, -0.022046559929893565, -0.025849811991035504, -0.018463944506982403, -0.02095125584232907, -0.019019377071074716, -0.013626128855073924, -0.02402019869311934, -0.08016249451473897]\n",
    "\n",
    "to get exact values for the weight we are using first code snippet. where in predict we are directly returning 0 or 1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat.txt\")\n",
    "instances_te = read_data(\"test.dat.txt\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "    for tr_size in tr_percent:\n",
    "        for epochs in num_epochs:\n",
    "            size =  round(len(instances_tr)*tr_size/100)\n",
    "            pre_instances = instances_tr[0:size]\n",
    "            weights = train_perceptron(pre_instances, lr, epochs)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3:\n",
    "Hyperparameter optimization is the process of finding the right combination of hyperparameter values to achieve max performance on your data in a Reasonable time.\n",
    "\n",
    "we can get results by using Grid Search, Random Search optimization:\n",
    "\n",
    "Grid Search:\n",
    " - a widely used method that performs to get the optimal values for a given model using hyperparameter tuning.\n",
    " - works by trying all possible combination of parameters you want to try in the model.\n",
    " - will take more time to perform the entire search due to this can get very computationally expensive.\n",
    " \n",
    "Random Search:\n",
    " - Works bit differently, compared to grid search. \n",
    " - random combinations of the values of the hyperparameters are used to find the best solution for the built models\n",
    " - Drawback: sometimes miss important values in the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "\n",
    "We don't need to train with all the training datasets to get the highest accuracy with testdata.\n",
    "\n",
    "- Having more data certainly will increase the accuracy of your model, but at certain point where even adding infinite of data cannot improve any more accuracy. this is called as natural noise of the data.\n",
    "\n",
    "- Placing too much emphasis on each data point, there needs to be deal with lot of noise, that cause lose sight of whats really important.\n",
    "\n",
    "- Conclusion: So adding more data points to the training set will not improve the model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "\n",
    "There might be chance of overfitting by providing more training data.\n",
    "\n",
    "- If the model is trained with too much data, then it could essentially memorize the data, which causes model overfitting, which causes a high error rate for unseen data. Due to this we get wrong predictions and lose focus on important data.\n",
    "\n",
    "- Sometimes we dont need more training data to get high accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C. Can you get higher accuracy with additional hyperparameters (higher than 80.0)?\n",
    "\n",
    "We can not get accuracy higher than 80.0 by adding additional hyperparameters. \n",
    "- when model is overfitting(performing well on a training set and poor on test dataset)\n",
    "- when model is underftting(performing poorly on tyhe training dataset and well on test dataset\n",
    "- optimizing hypperparameters can help, little tweak can make difference in getting accuracy from 60% to 80%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "Training for more epochs is worth when setting the number of epochs as high as possible and terminate the training when validation error start increasing.\n",
    "\n",
    "- Too many epochs can lead to overfitting of the training dataset\n",
    "- Too few epochs may result in an underfit model\n",
    "- Early stopping is a method that allows you to specify number of training epochs and stop training once the model performance stops improving on a hold out validation dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
